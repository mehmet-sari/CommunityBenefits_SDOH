{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Get IRS Data\n",
    "\n",
    "IRS Form Data is stored at AWS and it can be found [here](https://registry.opendata.aws/irs990/). The documentation is self-explanatory. Two things you need to know is:\n",
    "\n",
    "- IRS provides an index file for each year that contains all tax returns for that year. These index files includes basic information about each filing, including the name of the filer, the Employer Identification Number (EIN) of the filer, the date of the filing, and unique identifier for the filing. The index file for 2017, for example is can be found at [here]( https://s3.amazonaws.com/irs-form-990/index_2017.csv). For other year you just need to change year in the link.\n",
    "\n",
    "- Each URL link, which contains XML, has a unique identifier for that organization. For example http://s3.amazonaws.com/irs-form-990/201703199349311180_public.xml shows FOUNDATION HEALTH SYSTEMS CORP.'s return for 2017. 2017 is for year and 03199349311180 is a unique identifier for that organization. This whole number (OBJECT_ID) can be found at the last column of index file which I mention above.  \n",
    "\n",
    "Last but not least IRS XML structure has changed after 2013. Even though there are minor change in the element names it needs further process to get exact identification for that element if someone wants to create a panel data with pre-2013 and pro-2013 years.\n",
    "\n",
    "The code below shows how I get XML links for the hospitals with Schedule H for 2017 tax returns. I store all XML links in csv file to use it for further analysis. \n",
    "\n",
    "**PS**: All the codes below are not final so they are not clean code yet. There will be final (clean) version with many comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from xml.dom import minidom\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import requests\n",
    "data2016= pd.read_csv(\"https://s3.amazonaws.com/irs-form-990/index_2017.csv\")\n",
    "URLlist = []\n",
    "path=\"/home/msari/Project1/RawData/IRS/2016\"\n",
    "for i in data2016.iloc[:,-1]:\n",
    "    try:\n",
    "        URL=\"https://s3.amazonaws.com/irs-form-990/\"+str(i)+\"_public.xml\"\n",
    "        mydoc = minidom.parse(urlopen(URL))\n",
    "        scheduleH=mydoc.getElementsByTagName('IRS990ScheduleH')\n",
    "        if len(scheduleH) == 1:\n",
    "            URLlist.append(URL)\n",
    "            response=requests.get(URL)\n",
    "            with open(os.path.join(path,str(i)+\"_public.xml\"), 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            continue\n",
    "    except:\n",
    "        pass\n",
    "with open(\"2016output.csv\",'w') as r:\n",
    "    wr = csv.writer(r)\n",
    "    for url in URLlist:\n",
    "        wr.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I store all the XML links that I need, I need to convert XMLs to a dataframe whose unit level is hospital for that specific year. So the dataframe will be panel data. There are many ways to turn XMLs to dataframe, however, what I do below is simply get all elements and child elements as column name(variable name) without specifying in advance. For example CharityCareAtCost.NetCommunityBenefitExpense shows the each key under the ScheduleH unit. There are other categories that have same NetCommunityBenefitExpense key, so this method simply avoids duplicating and renaming process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import xmltodict\n",
    "from pandas.io.json import json_normalize\n",
    "for i in range(2010, 2018):\n",
    "    df = pd.read_csv(str(i)+\"output.csv\")\n",
    "    IRS_temp = pd.DataFrame([])\n",
    "    for url in df.iloc[:,0]:\n",
    "        URL=url\n",
    "        webpage = requests.get(URL)\n",
    "        units = xmltodict.parse(webpage.content)\n",
    "        ScheduleH = units['Return']['ReturnData']['IRS990ScheduleH']\n",
    "        ReturnHeader = units['Return']['ReturnHeader']\n",
    "        Profile = {**ScheduleH, **ReturnHeader}\n",
    "        Profile = json_normalize(Profile)\n",
    "        IRS_temp = IRS_temp.append(Profile,  sort= False)\n",
    "    IRS_temp.to_csv(\"IRS_temp_\"+str(i)+\".csv\")\n",
    "d = {}\n",
    "for i in range(2010, 2013):\n",
    "    temp = pd.read_csv(\"IRS_temp_\"+str(i)+\".csv\", low_memory = False)\n",
    "    temp = temp.assign(Year=i)\n",
    "    d[i] = temp.rename(columns={'TaxYear': 'TaxYr'}, inplace= True)\n",
    "    d[i] = temp.set_index(['Filer.EIN','TaxYr'], append=True)\n",
    "for i in range(2013, 2018):\n",
    "    temp = pd.read_csv(\"IRS_temp_\"+str(i)+\".csv\", low_memory= False)\n",
    "    temp = temp.assign(Year=i)\n",
    "    d[i] = temp.set_index(['Filer.EIN','TaxYr'], append=True)\n",
    "IRS990 = pd.concat([d[2010], d[2011], d[2012], d[2013],d[2014],d[2015],d[2016],d[2017]], sort=False)\n",
    "IRS990.to_csv(\"IRS990.csv\", index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
